<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
  <meta http-equiv="Content-Type" content="text/html" charset="UTF-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />


  <title>Neural Networks from a Bayesian Perspective</title>


  <meta name="HandheldFriendly" content="True" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="referrer" content="origin" />
  <meta name="generator" content="Pelican" />
<link href="https://anotherdatum.com/bnn.html" rel="canonical" />
  <!-- Feed -->
        <link href="https://anotherdatum.com/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="Another Datum Full Atom Feed" />

  <link href="https://anotherdatum.com/theme/css/style.css" type="text/css" rel="stylesheet" />

  <!-- Code highlight color scheme -->
      <link href="https://anotherdatum.com/theme/css/code_blocks/tomorrow.css" rel="stylesheet">

    <!-- CSS specified by the user -->


    <link href="https://anotherdatum.com/css/overrides.css" type="text/css" rel="stylesheet" />

  <!-- Custom fonts -->
  <link href='https://fonts.googleapis.com/css?family=Montserrat:400,300' rel='stylesheet' type='text/css' />
  <link href="https://fonts.googleapis.com/css?family=Lato" rel="stylesheet" type="text/css" />

  <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css" rel="stylesheet" type="text/css">
  <link href='https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
  <link href='https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800' rel='stylesheet' type='text/css'>

  <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
  <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
  <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
    <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
  <![endif]-->



    <meta name="description" content="Learn how to estimate model uncertainty in neural networks.">

    <meta name="author" content="Yoel Zeldes">

    <meta name="tags" content="deep learning">
    <meta name="tags" content="uncertainty">




<!-- Open Graph -->
<meta property="og:site_name" content="Another Datum"/>
<meta property="og:title" content="Neural Networks from a Bayesian Perspective"/>
<meta property="og:description" content="Learn how to estimate model uncertainty in neural networks."/>
<meta property="og:locale" content="en_US"/>
<meta property="og:url" content="https://anotherdatum.com/bnn.html"/>
<meta property="og:type" content="article"/>
<meta property="article:published_time" content="2018-08-06 21:00:00+03:00"/>
<meta property="article:modified_time" content=""/>
<meta property="article:author" content="https://anotherdatum.com/author/yoel-zeldes.html">
  <meta property="article:publisher" content="https://www.facebook.com/yoel.zeldes" />
<meta property="article:section" content="BNN"/>
<meta property="article:tag" content="deep learning"/>
<meta property="article:tag" content="uncertainty"/>
<meta property="og:image" content="https://anotherdatum.com/images/uncertainty_paper/BNN/cover.jpg">

<!-- Twitter Card -->
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:site" content="@YZeldes">
    <meta name="twitter:title" content="Neural Networks from a Bayesian Perspective">
    <meta name="twitter:url" content="https://anotherdatum.com/bnn.html">

        <meta name="twitter:image:src" content="https://anotherdatum.com/images/uncertainty_paper/BNN/cover.jpg">

      <meta name="twitter:description" content="Learn how to estimate model uncertainty in neural networks.">

<script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "Article",
  "name": "Neural Networks from a Bayesian Perspective",
  "headline": "Neural Networks from a Bayesian Perspective",
  "datePublished": "2018-08-06 21:00:00+03:00",
  "dateModified": "",
  "author": {
    "@type": "Person",
    "name": "Yoel Zeldes",
    "url": "https://anotherdatum.com/author/yoel-zeldes.html"
  },
  "image": "https://anotherdatum.com/images/uncertainty_paper/BNN/cover.jpg",
  "url": "https://anotherdatum.com/bnn.html",
  "description": "Learn how to estimate model uncertainty in neural networks."
}
</script>
</head>
<!-- TODO : Body class -->
<body class="home-template">

<nav id="menu">
  <a class="close-button">Close</a>
  <div class="nav-wrapper">
    <p class="nav-label">Menu</p>
    <ul>
          <li><a href="https://anotherdatum.com" role="presentation">Posts</a></li>

              <li role="presentation"><a href="https://anotherdatum.com/pages/about.html">about me</a></li>
              <li role="presentation"><a href="https://anotherdatum.com/pages/resources.html">Resources</a></li>

    </ul>
  </div>
</nav>
    <!-- Progressbar -->
    <div class="progress-container">
        <span class="progress-bar"></span>
    </div>

    <!-- Page Header -->
    <!-- Set your background image for this header on the line below. -->
    <header id="post-header" class="has-cover">
      <div class="inner">
        <nav id="navigation">
            <span id="home-button" class="nav-button">
                <a class="home-button" href="https://anotherdatum.com/" title="Home"><i class="ic ic-arrow-left"></i> Home</a>
            </span>
          <span id="menu-button" class="nav-button">
            <a class="menu-button"><i class="ic ic-menu"></i> Menu</a>
          </span>
        </nav>
        <h1 class="post-title">Neural Networks from a Bayesian Perspective</h1>
        <!-- TODO : Proper class for headline -->
        <span class="post-meta">
            <time datetime="06 August 2018">06 August 2018</time>
        </span>
        <!-- TODO : Modified check -->
            <div class="post-cover cover" style="background-image: url('https://anotherdatum.com/images/uncertainty_paper/BNN/cover.jpg')">
      </div>
    </header>

  <section id="wrapper">
    <a class="hidden-close"></a>

    <!-- Post content -->
    <main class="content" role="main">
        <article class="post">
        <div class="inner">
            <section class="post-content">
                <p><em>This is a joint post with <a href="https://medium.com/@inbarnaor">Inbar Naor</a>. Originally published at <a href="https://engineering.taboola.com/neural-networks-bayesian-perspective">engineering.taboola.com</a>.</em></p>
<hr>
<p>Understanding what a model doesn’t know is important both from the
practitioner’s perspective and for the end users of many different machine
learning applications. In <a href="https://engineering.taboola.com/using-uncertainty-interpret-model">our previous blog
post</a> we
discussed the different types of uncertainty. We explained how we can use it to
interpret and debug our models.</p>
<p>In this post we’ll discuss different ways to obtain uncertainty in Deep Neural
Networks. Let’s start by looking at neural networks from a Bayesian perspective.</p>
<h1>Bayesian learning 101</h1>
<p>Bayesian statistics allow us to draw conclusions based on both evidence (data)
and our prior knowledge about the world. This is often contrasted with
frequentist statistics which only consider evidence. The prior knowledge
captures our belief on which model generated the data, or what the weights of
that model are. We can represent this belief using a <em>prior distribution</em> <span class="math">\(p(w)\)</span>
over the model’s weights.</p>
<p>As we collect more data we update the prior distribution and turn in into a
<em>posterior distribution</em> using Bayes’ law, in a process called <em>Bayesian
updating</em>:</p>
<p><span class="math">\(p(w|X,Y) = \frac{p(Y|X,w) p(w)}{p(Y|X)}\)</span></p>
<p>This equation introduces another key player in Bayesian learning — the
<em>likelihood</em>, defined as <span class="math">\(p(y|x,w)\)</span>. This term represents how likely the data is,
given the model’s weights <span class="math">\(w\)</span>.</p>
<h1>Neural networks from a Bayesian perspective</h1>
<p>A neural network’s goal is to estimate the likelihood <span class="math">\(p(y|x,w)\)</span>. This is true
even when you’re not explicitly doing that, e.g. <a href="https://www.jessicayung.com/mse-as-maximum-likelihood">when you minimize
MSE</a>.</p>
<p>To find the best model weights we can use <em>Maximum Likelihood Estimation</em> (MLE):</p>
<div class="math">\begin{split}
w^{MLE} &amp;= \text{argmax}_{w}\text{log}P(D|w) \\
&amp; = \text{argmax}_{w}\sum_i \text{log}P(y_i|x_i,w)
\end{split}</div>
<p>Alternatively, we can use our prior knowledge, represented as a prior
distribution over the weights, and maximize the posterior distribution. This
approach is called <em>Maximum Aposteriori Estimation</em> (MAP):</p>
<div class="math">\begin{split}
w^{MAP} &amp;= \text{argmax}_{w}\text{log}P(w|D) \\
&amp; = \text{argmax}_{w}\text{log}P(D|w) + \text{log}P(w)
\end{split}</div>
<p>The term <span class="math">\(\text{log}P(w)\)</span>, which represents our prior, acts as a regularization term.
Choosing a Gaussian distribution with mean 0 as the prior, you’ll get the
mathematical equivalence of L2 regularization.</p>
<p>Now that we start thinking about neural networks as probabilistic creatures, we
can let the fun begin. For start, who says we have to output one set of weights
at the end of the training process? What if instead of learning the model’s
weights, we learn a distribution over the weights? This will allow us to
estimate uncertainty over the weights. So how do we do that?</p>
<h1>Once you go Bayesian, you never go back</h1>
<p>We start again with a prior distribution over the weights and aim at finding
their posterior distribution. This time, instead of optimizing the network’s
weights directly we’ll average over all possible weights (referred to as
marginalization).</p>
<p>At inference, instead of taking the single set of weights that maximized the
posterior distribution (or the likelihood, if we’re working with MLE), we
consider all possible weights, weighted by their probability. This is achieved
using an integral:</p>
<p><span class="math">\(p(y|x,X,Y) = {\displaystyle \int} p(y|x,w)p(w|X,Y)dw\)</span></p>
<p><span class="math">\(x\)</span> is a data point for which we want to infer <span class="math">\(y\)</span>, and <span class="math">\(X\)</span>,<span class="math">\(Y\)</span> are training
data. The first term <span class="math">\(p(y|x,w)\)</span> is our good old likelihood, and the second term
<span class="math">\(p(w|X,Y)\)</span> is the posterior probability of the model’s weights given the data.</p>
<p>We can think about it as an ensemble of models weighted by the probability of
each model. Indeed this is equivalent to an ensemble of infinite number of
neural networks, with the same architecture but with different weights.</p>
<h1>Are we there yet?</h1>
<p>Ay, There’s the rub! Turns out that this integral is intractable in most cases.
This is because the posterior probability cannot be evaluated analytically.</p>
<p>This problem is not unique to Bayesian Neural Networks. You would run into this
problem in many cases of Bayesian learning, and many methods to overcome this
have been developed over the years. We can divide these methods into two
families: variational inference and sampling methods.</p>
<p><img alt="" src="images/uncertainty_paper/BNN/methods.png"></p>
<h3>Monte Carlo sampling</h3>
<p>We have a problem. The posterior distribution is intractable. What if instead of
computing the integral over the true distribution we’ll approximate it with the
average of samples drawn from it? One way to do that is the <a href="https://towardsdatascience.com/a-zero-math-introduction-to-markov-chain-monte-carlo-methods-dcba889e0c50">Markov Chain Monte
Carlo</a>
— you construct a markov chain with the desired distribution as its equilibrium
distribution.</p>
<h3>Variational Inference</h3>
<p>Another solution is to approximate the true intractable distribution with a
different distribution from a tractable family. To measure the similarity of the
two distribution we can use KL divergence:</p>
<p><span class="math">\(D_{KL}(p||q) = {\displaystyle \int}_{-\infty}^{\infty} p(x) log \frac{p(x)}{q(x)}dx\)</span></p>
<p>Let <span class="math">\(q\)</span> be a variational distribution parameterized by <span class="math">\(\theta\)</span>. We want to find the
value of <span class="math">\(\theta\)</span> that minimizes the KL divergence:</p>
<div class="math">\begin{split}
\theta^* &amp;= \text{argmin}_\theta D_{KL}(q_\theta(w)||p(w|X,Y)) \\
&amp; = \text{argmin}_\theta {\displaystyle \int} q_\theta(w) log \frac{q_\theta(w)p(X,Y)}{p(X,Y|w)p(w)}dw \\
&amp; = \text{argmin}_\theta {\displaystyle \int} q_\theta(w) log \frac{q_\theta(w)}{p(w)}dw - {\displaystyle \int} q_\theta(w) log (p(X,Y|w))dw \\
&amp; = \text{argmin}_\theta D_{KL}(q_\theta(w)||p(w)) - \mathbb{E}_{q_\theta(w)} log(p(X,Y|w))
\end{split}</div>
<p>Look at what we’ve got: the first term is the KL divergence between the
variational distribution and the prior distribution. The second term is the
likelihood with regards to <span class="math">\(q_\theta\)</span>. So we’re looking for <span class="math">\(q_\theta\)</span> that explains the
data best, but on the other hand is as close as possible to the prior
distribution. This is just another way to introduce regularization into neural
networks!</p>
<p>Now that we have <span class="math">\(q_\theta\)</span> we can use it to make predictions:</p>
<p><span class="math">\(q_\theta(y|x) = {\displaystyle \int} p(y|x,w)q_\theta(w)dw\)</span></p>
<p>The above formulation comes from a <a href="http://proceedings.mlr.press/v37/blundell15.html">work by
DeepMind</a> in 2015. Similar
ideas were presented by
<a href="http://proceedings.mlr.press/v37/blundell15.html">graves</a> in 2011 and go back
to <a href="http://www.cs.toronto.edu/~fritz/absps/colt93.pdf">Hinton and van Camp</a> in
1993. The <a href="https://www.youtube.com/watch?v=FD8l2vPU5FY">keynote</a> in NIPS
Bayesian Deep Learning workshop had a very nice overview of how these ideas
evolved over the years.</p>
<p>OK, but what if we don’t want to train a model from scratch? What if we have a
trained model that we want to get uncertainty estimation from? Can we do that?</p>
<p>It turns out that if we use dropout during training, we actually can.</p>
<p><img alt="" src="images/uncertainty_paper/BNN/data_scientists.jpg">
<em>Professional data scientists contemplating the uncertainty of their model — an
illustration</em></p>
<h3>Dropout as a mean for uncertainty</h3>
<p><a href="http://jmlr.org/papers/v15/srivastava14a.html">Dropout</a> is a well used practice
as a regularizer. In training time, you randomly sample nodes and drop them out,
that is — set their output to 0. The motivation? You don’t want to over rely on
specific nodes, which might imply overfitting.</p>
<p>In 2016, <a href="http://proceedings.mlr.press/v48/gal16.pdf">Gal and Ghahramani</a> showed
that if you apply dropout at inference time as well, you can easily get an
uncertainty estimator:</p>
<ol>
<li>Infer <span class="math">\(y|x\)</span> multiple times, each time sample a different set of nodes to drop out.</li>
<li>Average the predictions to get the final prediction <span class="math">\(\mathbb{E}(y|x)\)</span>.</li>
<li>Calculate the sample variance of the predictions.</li>
</ol>
<p>That’s it! You got an estimate of the variance! The
<a href="http://www.cs.ox.ac.uk/people/yarin.gal/website/blog_3d801aa532c1ce.html">intuition</a>
behind this approach is that the training process can be thought of as training
<span class="math">\(2^m\)</span> different models simultaneously — where m is the number of nodes in the
network: each subset of nodes that is not dropped out defines a new model. All
models share the weights of the nodes they don’t drop out. At every batch, a
randomly sampled set of these models is trained.</p>
<p>After training, you have in your hands an ensemble of models. If you use this
ensemble at inference time as described above, you get the ensemble’s
uncertainty.</p>
<h1>Sampling methods vs Variational Inference</h1>
<p>In terms of the <a href="https://en.wikipedia.org/wiki/Biasâvariance_tradeoff">bias-variance
tradeoff</a>, variational
inference has high bias because we choose the distributions family. This is a
strong assumption that we’re making, and as any strong assumption, it introduces
bias. However, it’s stable, with low variance.</p>
<p>Sampling methods on the other hand have low bias, because we don’t make
assumptions about the distribution. This comes at the price of high variance,
since the result is dependent on the samples we draw.</p>
<hr>
<h1>Final thoughts</h1>
<p>Being able to estimate the model uncertainty is a hot topic. It’s important to
be aware of it in high risk applications such as medical assistants and
self-driving cars. It’s also a valuable tool to understand which data could
benefit the model, so we can go and get it.</p>
<p>In this post we covered some of the approaches to get model uncertainty
estimations. There are many more methods out there, so if you feel highly
uncertain about it, go ahead and look for more data 🙂</p>
<p>In the next post we’ll show you how to use uncertainty in recommender systems,
and specifically — how to tackle the <a href="https://en.wikipedia.org/wiki/Multi-armed_bandit">exploration-exploitation
challenge</a>. Stay tuned.</p>
<hr>
<p><em>This is the second post of a series related to a paper we’re presenting in a
workshop in this year KDD conference: <a href="https://arxiv.org/abs/1711.02487">deep density networks and uncertainty in
recommender systems</a></em>.</p>
<p><em>The first post can be found
<a href="https://engineering.taboola.com/using-uncertainty-interpret-model">here</a>.</em></p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'Loading awesomeness...'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
            </section>

            <section class="post-info">
                <div class="post-share">
                    <a class="twitter" href="https://twitter.com/share?text=Neural Networks from a Bayesian Perspective&amp;url=https://anotherdatum.com/bnn.html" onclick="window.open(this.href, 'twitter-share', 'width=550,height=235');return false;">
                    <i class="ic ic-twitter"></i><span class="hidden">Twitter</span>
                    </a>
                    <a class="facebook" href="https://www.facebook.com/sharer/sharer.php?u=https://anotherdatum.com/bnn.html" onclick="window.open(this.href, 'facebook-share','width=580,height=296');return false;">
                    <i class="ic ic-facebook"></i><span class="hidden">Facebook</span>
                    </a>
                    <div class="clear"></div>
                </div>

                <aside class="post-tags">
<a href="https://anotherdatum.com/tag/deep-learning.html">deep learning</a><a href="https://anotherdatum.com/tag/uncertainty.html">uncertainty</a>                </aside>

                <div class="clear"></div>


                </section>

<!-- Begin MailChimp Signup Form -->
<link href="//cdn-images.mailchimp.com/embedcode/classic-10_7.css" rel="stylesheet" type="text/css">
<style type="text/css">
	#mc_embed_signup{background:#fff; clear:left; font:14px Helvetica,Arial,sans-serif;  width:300px;}
	#mc_embed_signup form{padding: 0;}
	/* Add your own MailChimp form style overrides in your site stylesheet or in this style block.
	   We recommend moving this block and the preceding CSS link to the HEAD of your HTML file. */
</style>
<div id="mc_embed_signup">
<form action="https://anotherdatum.us14.list-manage.com/subscribe/post?u=6894d7badcfb253606fa3fb54&amp;id=c6f34ad6b7" method="post" id="mc-embedded-subscribe-form" name="mc-embedded-subscribe-form" class="validate" target="_blank" novalidate>
    <div id="mc_embed_signup_scroll">
	<h2>Get updated of new posts</h2>
<div class="mc-field-group">
	<label for="mce-EMAIL">Email Address </label>
	<input type="email" value="" name="EMAIL" class="required email" id="mce-EMAIL">
</div>
	<div id="mce-responses" class="clear">
		<div class="response" id="mce-error-response" style="display:none"></div>
		<div class="response" id="mce-success-response" style="display:none"></div>
	</div>    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->
    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_6894d7badcfb253606fa3fb54_c6f34ad6b7" tabindex="-1" value=""></div>
    <div class="clear"><input type="submit" value="Subscribe" name="subscribe" id="mc-embedded-subscribe" class="button"></div>
    </div>
</form>
</div>
<script type='text/javascript' src='//s3.amazonaws.com/downloads.mailchimp.com/js/mc-validate.js'></script><script type='text/javascript'>(function($) {window.fnames = new Array(); window.ftypes = new Array();fnames[0]='EMAIL';ftypes[0]='email';fnames[1]='FNAME';ftypes[1]='text';fnames[2]='LNAME';ftypes[2]='text';}(jQuery));var $mcj = jQuery.noConflict(true);</script>
<!--End mc_embed_signup-->
<hr />
                <aside class="post-nav">
                    <a class="post-nav-next" href="https://anotherdatum.com/exploration-exploitation.html">
                        <section class="post-nav-teaser">
                            <i class="ic ic-arrow-left"></i>
                                <h2 class="post-nav-title">Recommender Systems: Exploring the Unknown Using Uncertainty</h2>
                            <p class="post-nav-excerpt">Learn what the exploration-exploitation tradeoff is, and how to use your model's...</p>
                        </section>
                    </a>
                    <a class="post-nav-prev" href="https://anotherdatum.com/using-uncertainty-to-interpret-your-model.html">
                        <section class="post-nav-teaser">
                            <i class="ic ic-arrow-right"></i>
                                <h2 class="post-nav-title">Using Uncertainty to Interpret your Model</h2>
                            <p class="post-nav-excerpt">Interpreting deep learning models is hard. Learn how to use uncertainty estimates to...</p>
                        </section>
                    </a>
                    <div class="clear"></div>
                </aside>

                <div class="comments">
                    <h2>Comments !</h2>
                    <div id="disqus_thread"></div>
                    <script type="text/javascript">
                        var disqus_shortname = 'anotherdatum';
                        var disqus_identifier = 'bnn.html';
                        var disqus_url = 'https://anotherdatum.com/bnn.html';
                        (function() {
                            var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
                            dsq.src = '//anotherdatum.disqus.com/embed.js';
                            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
                        })();
                    </script>
                    <noscript>Please enable JavaScript to view the comments.</noscript>
                </div>
            </div>
        </article>
    </main>
      <!-- TODO : Body class -->
    <div id="body-class" style="display: none;" class=""></div>

    <footer id="footer">
            <div class="social">
                <a href="https://il.linkedin.com/in/yoelzeldes">
                    <span class="fa-stack fa-lg">
                        <i class="fa fa-circle fa-stack-2x"></i>
                        <i class="fa fa-linkedin fa-stack-1x fa-inverse"></i>
                    </span>
                </a>
                <a href="https://github.com/yoel-zeldes">
                    <span class="fa-stack fa-lg">
                        <i class="fa fa-circle fa-stack-2x"></i>
                        <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                    </span>
                </a>
                <a href="https://www.facebook.com/yoel.zeldes">
                    <span class="fa-stack fa-lg">
                        <i class="fa fa-circle fa-stack-2x"></i>
                        <i class="fa fa-facebook fa-stack-1x fa-inverse"></i>
                    </span>
                </a>
                <a href="https://twitter.com/YZeldes">
                    <span class="fa-stack fa-lg">
                        <i class="fa fa-circle fa-stack-2x"></i>
                        <i class="fa fa-twitter fa-stack-1x fa-inverse"></i>
                    </span>
                </a>
            </div>

      <div class="inner">
        <section class="credits">
          <span class="credits-theme">Have a look at <a href="https://github.com/yoel-zeldes/yoel-zeldes.github.io/tree/source">the source code</a> of this blog.</span>
        </section>
      </div>
    </footer>
  </section>

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.4.1/jquery.min.js"></script>
  <script type="text/javascript" src="https://anotherdatum.com/theme/js/script.js"></script>

    <!-- Global Site Tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-83684090-1"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'UA-83684090-1', { 'anonymize_ip': true });
    </script>
<script type="text/javascript">
    var disqus_shortname = 'anotherdatum';
    (function () {
        var s = document.createElement('script'); s.async = true;
        s.type = 'text/javascript';
        s.src = '//' + disqus_shortname + '.disqus.com/count.js';
        (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
    }());
</script>
</body>
</html>